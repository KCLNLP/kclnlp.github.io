Index,Person in charge,Model name,Source code description,Model figure,Source code URL,Impact of the model proposed,Dataset Name,Dataset description,Dataset URL link,Paper citation,Paper pdf URL
1,Lixing Zhu,VADet -- Vaccine Attitude Detection model,"

A latent representation learning model that jointly learns a stance classifier and disentangles the latent variables capturing stance and aspect respectively.",,https://github.com/somethingx1202/VADet," 14 paper citations, 16 stars, 3 forks","VAD -- Vaccine Attitude Dataset
","
The dataset contains 2,800 tweets relating to COVID-19 vaccines. Each tweet is annotated with a stance label and a text span characterizing the aspect. 


",https://github.com/somethingx1202/VADet/tree/main/Datasets_Raw,"@inproceedings{zhu-etal-2022-disentangled,
    title = ""Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media"",
    author = ""Zhu, Lixing  and
      Fang, Zheng  and
      Pergola, Gabriele  and
      Procter, Robert  and
      He, Yulan"",
    booktitle = ""Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jul,
    year = ""2022"",
    address = ""Seattle, United States"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.naacl-main.112"",
    doi = ""10.18653/v1/2022.naacl-main.112"",
    pages = ""1566--1580"",
    abstract = ""Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection, called VADet. A variational autoencoding architecture based on language models is employed to learn from unlabelled data the topical information of the domain. Then, the model is fine-tuned with a few manually annotated examples of user attitudes. We validate the effectiveness of VADet on our annotated data and also on an existing vaccination corpus annotated with opinions on vaccines. Our results show that VADet is able to learn disentangled stance and aspect topics, and outperforms existing aspect-based sentiment analysis models on both stance detection and tweet clustering."",
}
",https://aclanthology.org/2022.naacl-main.112.pdf
2,Lixing Zhu,Joint Topic Word-embedding (JTW) Model,This repository contains the source code for joint learning topics and topic-specific word embeddings.,,https://github.com/somethingx02/topical_wordvec_models,"17 paper citations, 7 stars, 2 forks",,,,"@article{zhu-etal-2020-neural,
    title = ""A Neural Generative Model for Joint Learning Topics and Topic-Specific Word Embeddings"",
    author = ""Zhu, Lixing  and
      He, Yulan  and
      Zhou, Deyu"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.31"",
    doi = ""10.1162/tacl_a_00326"",
    pages = ""471--485"",
    abstract = ""We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents, a word is generated by a hidden semantic vector encoding its contextual semantic meaning, and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared with existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification."",
}",https://aclanthology.org/2020.tacl-1.31/
3,Jun Wang,XProNet: Cross-Modal Prototype Driven Network,"
A cross-modal prototype driven network to promote cross-modal pattern learning and exploit it to improve the task of radiology report generation. 
Apache 2.0 license.",,https://github.com/Markin-Wang/XProNet,"31 paper citation, 57 stars, 8 forks",,,,"@inproceedings{wang2022cross,
  title={Cross-Modal Prototype Driven Network for Radiology Report Generation},
  author={Wang, Jun and Bhalerao, Abhir and He, Yulan},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXV},
  pages={563--579},
  year={2022}
}",https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136950558.pdf
4,Xingwei Tan,Extracting Event Temporal Relations via Hyperbolic Geometry,This repository contains the source code for hyperbolic-based representation learning methods for Event TempRel Extraction. GPL-3.0 license,,https://github.com/Xingwei-Warwick/hyper-event-TempRel,"22 paper citation, 10 stars, 3 fork",,,,"@inproceedings{tan-etal-2021-extracting,
    title = ""Extracting Event Temporal Relations via Hyperbolic Geometry"",
    author = ""Tan, Xingwei  and
      Pergola, Gabriele  and
      He, Yulan"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.636"",
    doi = ""10.18653/v1/2021.emnlp-main.636"",
    pages = ""8065--8077"",
    abstract = ""Detecting events and their evolution through time is a crucial task in natural language understanding. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the Euclidean space and train a classifier to detect temporal relations between event pairs. However, embeddings in the Euclidean space cannot capture richer asymmetric relations such as event temporal relations. We thus propose to embed events into hyperbolic spaces, which are intrinsically oriented at modeling hierarchical structures. We introduce two approaches to encode events and their temporal relations in hyperbolic spaces. One approach leverages hyperbolic embeddings to directly infer event relations through simple geometrical operations. In the second one, we devise an end-to-end architecture composed of hyperbolic neural units tailored for the temporal relation extraction task. Thorough experimental assessments on widely used datasets have shown the benefits of revisiting the tasks on a different geometrical space, resulting in state-of-the-art performance on several standard metrics. Finally, the ablation study and several qualitative analyses highlighted the rich event semantics implicitly encoded into hyperbolic spaces."",
}",https://aclanthology.org/2021.emnlp-main.636/
5,Junru Lu,TranCLR,"Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation

A novel QA model with contrastive learning and invertible event transformation for addressing event-centric QA. The repository is under MIT license.",,https://github.com/LuJunru/TranCLR,7 citations,,,,"@article{lu2022event,
  title={Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation},
  author={Lu, Junru and Tan, Xingwei and Pergola, Gabriele and Gui, Lin and He, Yulan},
  journal={arXiv preprint arXiv:2210.12902},
  year={2022}
}",https://arxiv.org/abs/2210.12902
6,Hanqi Yan,Knowledge-Aware Graph (KAG) Model,"The repository contains the inplementation of paper ""Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction"".                                    Commonsense Knowledge, i.e., ConceptNet is applied as invariant feature to tackle the distribution shift and Position Bias.",,https://github.com/hanqi-qi/Position-Bias-Mitigation-in-Emotion-Cause-Analysis,"ACL21 Oral, 45 citations, 19stars, 7 forks",,,,"@inproceedings{yan-etal-2021-position,
    title = ""Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction"",
    author = ""Yan, Hanqi  and
      Gui, Lin  and
      Pergola, Gabriele  and
      He, Yulan"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-long.261"",
    doi = ""10.18653/v1/2021.acl-long.261"",
    pages = ""3364--3375"",
    abstract = ""The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing models for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing models on such adversarial examples and observe a significant performance drop. To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models."",
}",https://aclanthology.org/2021.acl-long.261/
7,Hanqi Yan,SoftDecay,"The repository contains the inplementation of paper ""Addressing Token Uniformity in Transformers via Singular Value Transformation"".                                                                  Token uniformity implies more vanished dimensions in the embedding space. _SoftDecay_ is proposed to a range of transformer-based language models and improved performance is observed in STS evaluation and a range of GLUE tasks.",,https://github.com/hanqi-qi/tokenUni,"UAI22 Spotlight, 10 citatations, 9 stars, 0 forks",,,,"@inproceedings{yan2022addressing,
  title={Addressing token uniformity in transformers via singular value transformation},
  author={Yan, Hanqi and Gui, Lin and Li, Wenjie and He, Yulan},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={2181--2191},
  year={2022},
  organization={PMLR}
}",https://openreview.net/pdf?id=BtUxE_8i5l5
8,Hanqi Yan,Hierarchical Interpretable Neural Text Classifier (Hint),"The repository contains the inplementation of paper ""Hierarchical Interpretation of Neural Text Classification"".                                                                                            Unsupervised self-explanatory framework for document classification. It can extract word-, sentence-, and topic-level rationales explaining the document-level decision.",,https://github.com/hanqi-qi/HINT,"Computation Linguistics 22, 11 citations, 2 stars, 0 forks",,,,"@article{10.1162/coli_a_00459,
    author = {Yan, Hanqi and Gui, Lin and He, Yulan},
    title = ""{Hierarchical Interpretation of Neural Text Classification}"",
    journal = {Computational Linguistics},
    pages = {1-34},
    year = {2022},
    month = {11},
    issn = {0891-2017},
    doi = {10.1162/coli_a_00459},
    url = {https://doi.org/10.1162/coli\_a\_00459},
    eprint = {https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli\_a\_00459/2058271/coli\_a\_00459.pdf},
}",https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00459/112768/Hierarchical-Interpretation-of-Neural-Text
9,Runcong Zhao,"dynamic
Brand-Topic Model (dBTM),","The repository contains the inplementation of paper ""Tracking Brand-Associated Polarity-Bearing Topics in User Reviews """,,https://github.com/BLPXSPG/dBTM,"1 citation, 1 star, 1 fork",,,,"@article{10.1162/coli_a_00459,
    author = {Runcong Zhao and Lin Gui and Hanqi Yan and Yulan He},
    title = ""{Tracking Brand-Associated Polarity-Bearing Topics in User Reviews}"",
    journal = {Transactions of the Association for Computational Linguistics},
    year = {2023},
  }",https://arxiv.org/abs/2301.07183
10,Xingwei Tan,"Bayesian
Translational model (Bayesian-Trans",The repository contains the Bayesian Learning-based Tranlation model for Event TempRel Extraction. MIT license,,https://github.com/Xingwei-Warwick/Bayesian-Trans,"2 paper citation, 8 star, 0 fork",,,,"@article{tan2023event,
  title={Event Temporal Relation Extraction with Bayesian Translational Model},
  author={Tan, Xingwei and Pergola, Gabriele and He, Yulan},
  journal={arXiv preprint arXiv:2302.04985},
  year={2023}
}",https://arxiv.org/abs/2302.04985
11,Junru Lu,NapSS Narrative Prompting and Sentencematching Summarization,"We propose a summarize-then-simplify two-stage strategy, which we call NapSS, identifying the relevant content to simplify while ensuring that the original narrative flow is preserved. In this approach, we first generate reference summaries via sentence matching between the original and the simplified abstracts.  Then, to ensure the narrative consistency of the simplified text, we synthesize auxiliary narrative prompts combining key phrases derived from the syntactical analyses of the original text.",,https://github.com/LuJunru/NapSS,10 citations,NapSS Dataset,"Napss conducted a manual evaluation of the outputs generated by the simplification models to provide additional insights into simplicity, fluency and factuality. Total 200 instances evaluated by 6 trained annotators and 2 domain expert annotators.",https://doi.org/10.5281/zenodo.7690014,"@article{lu2023napss,
  title={NapSS: Paragraph-level Medical Text Simplification via Narrative Prompting and Sentence-matching Summarization},
  author={Lu, Junru and Li, Jiazheng and Wallace, Byron C and He, Yulan and Pergola, Gabriele},
  journal={arXiv preprint arXiv:2302.05574},
  year={2023}
}",https://arxiv.org/abs/2302.05574
12,Zhaoyue Sun,PHEE,"The repository contains the source codes and dataset for the paper: ""PHEE: A Dataset for Pharmacovigilance Event Extraction from Text"".  The code includes the implementation of event extraction baselines on the PHEE dataset, i.e., a sequence labelling model, an extractive QA model and a generative QA model.",,https://github.com/ZhaoyueSun/PHEE,"2 citations, 8 stars, 2 forks.",PHEE Dataset,"The PHEE dataset contains over 5,000 finely annotated pharmacovigilance events from public medical case reports. Two types of events, the adverse events and the potential therapeutic events, are annotated. For each event, we annotate the event trigger and hierarchical arguments.  The main arguments (coarse-grained spans) include subject, treatment and effect. Further fine-grained sub-arguments - age, gender, race, number of patients (labelled as population) and preexisting conditions (labelled as subject.disorder) for the subject argument and drug (and their combinations), dosage, frequency, route, time-elapsed, duration, target disorder (labelled as treatment.disorder) for the treatment argument - are then annotated upon main arguments.",https://doi.org/10.5281/zenodo.7689970,"@article{sun2022phee,
  title={PHEE: A Dataset for Pharmacovigilance Event Extraction from Text},
  author={Sun, Zhaoyue and Li, Jiazheng and Pergola, Gabriele and Wallace, Byron C and John, Bino and Greene, Nigel and Kim, Joseph and He, Yulan},
  journal={arXiv preprint arXiv:2210.12560},
  year={2022}
}",https://arxiv.org/pdf/2210.12560.pdf
13,Jiazheng Li,CUE PLM-based Classifier Uncertainty Interpretation framework,"
A PLM-based text classifier uncertainty interpretation framework based on a variational autoencoder to estimate and interpret the source of the uncertainty at the token level.",,https://github.com/lijiazheng99/CUE,"2 citations, 8 stars",,,,"@inproceedings{
li2023cue,
title={{CUE}: An Uncertainty Interpretation Framework for Text Classifiers Built on Pre-Trained Language Models},
author={Jiazheng Li and ZHAOYUE SUN and Bin Liang and Lin Gui and Yulan He},
booktitle={The 39th Conference on Uncertainty in Artificial Intelligence},
year={2023},
url={https://openreview.net/forum?id=1G_WUgM1pnm}
}",https://openreview.net/pdf?id=1G_WUgM1pnm
14,Jiazheng Li,AERA Automated Explainable Student Response Assessment Framework,"
A rationale generation framework that ultizes LLM's few-shot capabilities on providing explainable assessment feedbacks on student answers data.",,https://github.com/lijiazheng99/aera,3 citations,,,,"@inproceedings{li-etal-2023-distilling,
    title = ""Distilling {C}hat{GPT} for Explainable Automated Student Answer Assessment"",
    author = ""Li, Jiazheng  and
      Gui, Lin  and
      Zhou, Yuxiang  and
      West, David  and
      Aloisi, Cesare  and
      He, Yulan"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2023"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.findings-emnlp.399"",
    doi = ""10.18653/v1/2023.findings-emnlp.399"",
    pages = ""6007--6026"",
}",https://aclanthology.org/2023.findings-emnlp.399.pdf
15,Jiazheng Li,OverPrompt ,"OverPrompt is a prompting technique that leverages the in-context learning capability of LLMs to handle multiple task inputs, thereby reducing token and time costs for each query. ",,https://github.com/lijiazheng99/OverPrompt,5 citations,,,,"@inproceedings{
li2023overprompt,
title={OverPrompt: Enhancing Chat{GPT} through Efficient In-Context Learning},
author={Jiazheng Li and Runcong Zhao and Yongxin Yang and Yulan He and Lin Gui},
booktitle={R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models},
year={2023},
url={https://openreview.net/forum?id=7jmtHtv9Ch}
}",https://openreview.net/pdf?id=7jmtHtv9Ch
16,Jiazheng Li,Thought Tree Guided Rationale Generation Framework,We propose a novel framework capable of generating more faithful rationales by mimicking the human assessment process by querying LLMs to generate a thought tree. We then summarise intermediate assessment decisions from each thought tree path for creating synthetic rationale data and rationale preference data.,,https://github.com/lijiazheng99/thought_tree_assessment,5 citations,,,,"@misc{li2024thoughttree,
        title={Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring}, 
        author={Jiazheng Li and Hainiu Xu and Zhaoyue Sun and Yuxiang Zhou and David West and Cesare Aloisi and Yulan He},
        year={2024},
        eprint={2406.19949},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2406.19949}, 
  }",https://aclanthology.org/2024.findings-emnlp.313.pdf 
17,Hainiu Xu,OpenToM -- Theory-of-Mind Benchmark Dataset,"
A benchmark for evaluating LLMs' theory-of-mind capabilities in reasoning about the physical as well as the psychological world from a character-centric perspective.

",,https://github.com/seacowx/OpenToM,"28 citations, 21 stars, 2 fork",OpenToM -- Thoery-of-Mind benchmarks,"
The dataset contains 696 narratives (100 of which are long narratives) depicting a simple scene in real life. Each narrative is accompanied by 23 first-order and second-order theory-of-mind questions. In total, there are 13,708 questions.  ",https://huggingface.co/datasets/SeacowX/OpenToM,"@inproceedings{xu-etal-2024-opentom,
    title = ""{O}pen{T}o{M}: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models"",
    author = ""Xu, Hainiu  and
      Zhao, Runcong  and
      Zhu, Lixing  and
      Du, Jinhua  and
      He, Yulan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.466/"",
    doi = ""10.18653/v1/2024.acl-long.466"",
    pages = ""8593--8623"",
    abstract = ""Neural Theory-of-Mind (N-ToM), machine`s ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.""
}",https://arxiv.org/pdf/2402.06044.pdf
18,Runcong Zhao,Conan,"Source code for ""Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives""",,https://github.com/BLPXSPG/Conan,"0 citation, 0 star, 0 fork",Conan,role-oriented relationships from the perspectives of various characters,https://github.com/BLPXSPG/Conan,"@article{zhao2024conan,
            title={Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives},
            author={Zhao, Runcong and Zhu, Qinglin and Xu, Hainiu and Li, Jiazheng and Zhou, Yuxiang and He, Yulan and Gui, Lin},
            journal={arXiv preprint arXiv:2402.11051},
            year={2024}
            }",https://arxiv.org/abs/2402.11051
19,Junru Lu,MemoChat,"MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation

We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. The repository is under MIT license.",,https://github.com/LuJunru/MemoChat,4 citations,MemoChat Dataset,"insturction tuning data constructed from public data, as well as small human-annotated testing set, can be found in source codes",https://github.com/LuJunru/MemoChat/tree/main/data,"@misc{lu2023memochat,
      title={MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation}, 
      author={Junru Lu and Siyu An and Mingbao Lin and Gabriele Pergola and Yulan He and Di Yin and Xing Sun and Yunsheng Wu},
      year={2023},
      eprint={2308.08239},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}",https://arxiv.org/abs/2308.08239
20,Junru Lu,"FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema
","
We present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema, for effective prompt optimization. The repository is under MIT license.",,https://github.com/LuJunru/FIPO_Project,0 citations,FIPO Dataset,30k preference samples for Automatic Prompt Optimization,https://huggingface.co/datasets/Junrulu/Prompt_Preference_Dataset,"@misc{lu2024fipo,
      title={FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema}, 
      author={Junru Lu and Siyu An and Min Zhang and Yulan He and Di Yin and Xing Sun},
      year={2024},
      eprint={2402.11811},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}",https://arxiv.org/abs/2402.11811
21,Itao Da Silva,Challenges in the Evaluation of the Causal Event Extraction Task,"Sourcce code and data for ""Challenges in the Evaluation of the Causal Event Extraction Task""",,https://github.com/oyarsa/event_extraction,,Challenges in the Evaluation of the Causal Event Extraction Task,Synthetic data based on the Fine-grained Causal Relations dataset for training entailment models.,https://github.com/oyarsa/event_extraction,,
22,Jun Wang,CAMANet,"CAMANet: Class Activation Map Guided Attention Network

A model which explicitly promotes cross-modal alignment by employing aggregated class activation maps to supervise cross-modal attention learning, and simultaneously enrich the discriminative information for radiology report generation. ",,https://github.com/Markin-Wang/CAMANet,"4 citations, 3 stars",,,,"@article{wang2024camanet,
  title={CAMANet: class activation map guided attention network for radiology report generation},
  author={Wang, Jun and Bhalerao, Abhir and Yin, Terry and See, Simon and He, Yulan},
  journal={IEEE Journal of Biomedical and Health Informatics},
  year={2024},
  publisher={IEEE}
}",https://ieeexplore.ieee.org/abstract/document/10400776
23,Xinyu Wang,"Proxy
Nodes Clustering Network (ProCNet)","Code for the paper: ""Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization""

An approach for document-level multi-event extraction with event proxy nodes and Hausdorff distance minimization. The event proxy nodes, representing pseudo-events, are able to build connections with other event proxy nodes, essentially capturing global information. The Hausdorff distance makes it possible to compare the similarity between the set of predicted events and the set of ground-truth events. By directly minimizing Hausdorff distance, the model is trained towards the global optimum directly, which improves performance and reduces training time.",,https://github.com/xnyuwg/procnet,2 citations,,,,"@inproceedings{wang-etal-2023-document,
    title = ""Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization"",
    author = ""Wang, Xinyu  and
      Gui, Lin  and
      He, Yulan"",
    booktitle = ""Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.acl-long.563"",
    pages = ""10118--10133"",
}",https://aclanthology.org/2023.acl-long.563.pdf
24,Xinyu Wang,construction-modellingmodification (CMM),"Resources for the paper: ""A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports""

A framework for Toc extraction, consisting of three steps: (1) Constructing an initial tree of text blocks based on reading order and font sizes; (2) Modelling each tree node (or text block) independently by considering its contextual information captured in node-centric subtree; (3) Modifying the original tree by taking appropriate action on each tree node (Keep, Delete, or Move). This construction-modellingmodification (CMM) process offers several benefits. It eliminates the need for pairwise modelling of section headings as in previous approaches, making document segmentation practically feasible. By incorporating structured information, each section heading can leverage both local and long-distance context relevant to itself.",,https://github.com/xnyuwg/cmm,1 citations,ESG dataset,"An ESG dataset comprises 1,093 publicly available ESG annual reports, sourced from 563 distinct
companies, and spans the period from 2001 to 2022.
The reports vary in length, ranging from 4 pages to
521 pages, with an average of 72 pages.",https://github.com/xnyuwg/cmm,"@inproceedings{wang-etal-2023-scalable,
    title = ""A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports"",
    author = ""Wang, Xinyu  and
      Gui, Lin  and
      He, Yulan"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
}",https://aclanthology.org/2023.emnlp-main.816.pdf
25,Yuxiang Zhou,DIVA architecture,"- DIVA -- Disentangling Interaction of VAriables framework  for causal inference

A framework tailored to mitigate the bias issue by unveiling interactions between different variables to disentangle the non-confounding covariates when estimating causal effects from text.",,https://github.com/zyxnlp/DIVA?tab=readme-ov-file,1 star,DIVA Dataset,"Semi-synthetic data were constructed for estimating causal effects from observational textual data. We collected transcripts from companies across twelve different sectors for earnings calls held between May 2001 and October 2019. The dataset was constructed to analyze two distinct treatment variables—political risk and sentiment—under two separate scenarios: stock volatility and stock movement. We split the dataset into training, validation, and test sets in an 8:1:6 ratio.",https://drive.google.com/file/d/1Cphjnh1VGnTeA76hWohOG1RfeQLT1BLH/view,"@inproceedings{zhou2023causal,
 title={Causal Inference from Text: Unveiling Interactions between Variables},
 author={Zhou, Yuxiang and He, Yulan},
 booktitle={Findings of EMNLP},
 year={2023},
 url={https://arxiv.org/pdf/2311.05286.pdf}
}",https://arxiv.org/pdf/2311.05286.pdf
26,Hanqi Yan,"Transformation based Adaptation for
Ratio bAlanced (TARA)","Repository for the paper ""Distinguishability Calibration to In-Context Learning"" appear in EACL 2023-findings.                                                                                                        Token uniformity/Information difussion issue is still observed in in-context learning, we proposed an adaptor for more discriminative representation learning and improved performance is observed in fine-grained text classification tasks.",,https://github.com/donttal/TARA,"2 citations, 4 stars, 1 forks",,,,"@inproceedings{li-etal-2023-distinguishability,
    title = ""Distinguishability Calibration to In-Context Learning"",
    author = ""Li, Hongjing  and
      Yan, Hanqi  and
      Li, Yanran  and
      Qian, Li  and
      He, Yulan  and
      Gui, Lin"",
    editor = ""Vlachos, Andreas  and
      Augenstein, Isabelle"",
    booktitle = ""Findings of the Association for Computational Linguistics: EACL 2023"",
    month = may,
    year = ""2023"",
    address = ""Dubrovnik, Croatia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.findings-eacl.102"",
    doi = ""10.18653/v1/2023.findings-eacl.102"",
    pages = ""1385--1397"",
    abstract = ""Recent years have witnessed increasing interests in prompt-based learning in which models can be trained on only a few annotated instances, making them suitable in low-resource settings. It is even challenging in fine-grained classification as the pre-trained language models tend to generate similar output embedding which makes it difficult to discriminate for the prompt-based classifier. In this work, we alleviate this information diffusion issue by proposing a calibration method based on a transformation which rotates the embedding feature into a new metric space where we adapt the ratio of each dimension to a uniform distribution to guarantee the distinguishability of learned embeddings. Furthermore, we take the advantage of hyperbolic embedding to capture the relation between dimensions by a coarse-fine metric learning strategy to enhance interpretability. Extensive experiments on the three datasets under various settings demonstrate the effectiveness of our approach."",
}",https://arxiv.org/abs/2302.06198
27,Hanqi Yan,Geometric InformAtioN boTtleneck (GIANT),"Repository for the paper ""Explainable Recommender with Geometric Information Bottleneck"" appear in TKDE 2024.                                                                                                  To ease the humman annotation for rationales in Recommender, a prior from user-item interactions is incorporated into the textual latent factors for explaination generation.",,N/A,1 citations,,,,"@article{yan2024explainable,
  title={Explainable Recommender with Geometric Information Bottleneck},
  author={Yan, Hanqi and Gui, Lin and Wang, Menghan and Zhang, Kun and He, Yulan},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}",https://arxiv.org/abs/2305.05331
28,Hanqi Yan,"Counterfactual Generation with Identifiability
Guarantees","Repository for the paper ""Counterfactual Generation with Identifiability Guarantees"" appear in Neurips 2023.                                                                                                        Provide Identification guarantees for successful disentanglement of the content and style variables, further supports the intervention of latent attributes of the text. This principled representations can shed light on the constrained, i.e., safe and moral generation for large language models with noisy pertaining data.",,https://github.com/hanqi-qi/Matte_Multi-domain-Counterfactual-Generation-with-identifiability-guarantee.git,"1 citations, 2 star",,,,"@inproceedings{
yan2023counterfactual,
title={Counterfactual Generation with Identifiability Guarantees},
author={Hanqi Yan and Lingjing Kong and Lin Gui and Yuejie Chi and Eric Xing and Yulan He and Kun Zhang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=cslnCXE9XA}
}
",https://openreview.net/pdf?id=cslnCXE9XA
29,Lixing Zhu,Disentangled Opinion Clustering (DOC),"Repository for the paper ""Disentangling Aspect and Stance via a Siamese Autoencoder for Aspect Clustering of Vaccination Opinions"" appears in  Findings of ACL 2023",,https://github.com/somethingx1202/DOC,3 citations,,,,"@inproceedings{zhu-etal-2023-disentangling,
    title = ""Disentangling Aspect and Stance via a {S}iamese Autoencoder for Aspect Clustering of Vaccination Opinions"",
    author = ""Zhu, Lixing  and
      Zhao, Runcong  and
      Pergola, Gabriele  and
      He, Yulan"",
    editor = ""Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL 2023"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.findings-acl.115"",
    doi = ""10.18653/v1/2023.findings-acl.115"",
    pages = ""1827--1842"",
    abstract = ""Mining public opinions about vaccines from social media has been increasingly relevant to analyse trends in public debates and to provide quick insights to policy-makers. However, the application of existing models has been hindered by the wide variety of users{'} attitudes and the new aspects continuously arising in the public debate. Existing approaches, frequently framed via well-known tasks, such as aspect classification or text span detection, make direct usage of the supervision information constraining the models to predefined aspect classes, while still not distinguishing those aspects from users{'} stances. As a result, this has significantly hindered the dynamic integration of new aspects. We thus propose a model, namely Disentangled Opinion Clustering (DOC), for vaccination opinion mining from social media. DOC is able to disentangle users{'} stances from opinions via a disentangling attention mechanism and a Swapping-Autoencoder, and is designed to process unseen aspect categories via a clustering approach, leveraging clustering-friendly representations induced by out-of-the-box Sentence-BERT encodings and disentangling mechanisms. We conduct a thorough experimental assessment demonstrating the benefit of the disentangling mechanisms and cluster-based approach on both the quality of aspect clusters and the generalization across new aspect categories, outperforming existing methodologies on aspect-based opinion mining."",
}",https://aclanthology.org/2023.findings-acl.115.pdf
30,Lixing Zhu,LLMLINK,"Repository for the paper ""LLMLINK: Dual LLMs for Dynamic Entity Linking on Long Narratives with Collaborative Memorisation and Prompt Optimisation"" appears in  COLING2025",,https://github.com/somethingx1202/LlmLink,0 citation,,,,"@inproceedings{zhu-etal-2025-llmlink,
    title = ""{L}lm{L}ink: Dual {LLM}s for Dynamic Entity Linking on Long Narratives with Collaborative Memorisation and Prompt Optimisation"",
    author = ""Zhu, Lixing  and
      Wang, Jun  and
      He, Yulan"",
    editor = ""Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven"",
    booktitle = ""Proceedings of the 31st International Conference on Computational Linguistics"",
    month = jan,
    year = ""2025"",
    address = ""Abu Dhabi, UAE"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2025.coling-main.751/"",
    pages = ""11334--11347""
}",https://aclanthology.org/2025.coling-main.751.pdf
31,Jun Wang,PromptRRG: Can Prompt Learning Benefit Radiology Report Generation?,"
A model that utilizes prompt learning to activate a pretrained model and incorporate prior knowledge for radiology report generation. ",,N/A,1 citation,,,,"@article{wang2023can,
  title={Can Prompt Learning Benefit Radiology Report Generation?},
  author={Wang, Jun and Zhu, Lixing and Bhalerao, Abhir and He, Yulan},
  journal={arXiv preprint arXiv:2308.16269},
  year={2023}
}",https://arxiv.org/pdf/2308.16269.pdf
32,Wenjia Zhang,BTIC -- Bert-based Multimodal Fake News Detection Framework,"
A Bert-based multimodal fake news detection framework, which captures both textual and visual infomration from unreliable news articles utilising the contrastive learning strategy.",,https://github.com/WenjiaZh/BTIC,"22 paper citations, 9 stars, 3 forks",,,,"@inproceedings{zhang2021supervised,
  title={Supervised contrastive learning for multimodal unreliable news detection in covid-19 pandemic},
  author={Zhang, Wenjia and Gui, Lin and He, Yulan},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={3637--3641},
  year={2021}
}",https://arxiv.org/pdf/2109.01850.pdf
33,Wenjia Zhang,NewsQuote — News Source and Quotation Dataset,"
A dataset built on quote extraction and attribution for expert recommendation, consisting of 24,031 quote-speaker pairs that appeared on a COVID-19 news corpus.",,https://github.com/WenjiaZh/NewsQuote,"2 paper citation, 2 stars",NewsQuote — News Source and Quotation Dataset,"A dataset built on quote extraction and attribution for expert recommendation, consisting of 24,031 quote-speaker pairs that appeared on a COVID-19 news corpus.",https://github.com/WenjiaZh/NewsQuote,"@article{zhang2023newsquote,
  title={NewsQuote: A Dataset Built on Quote Extraction and Attribution for Expert Recommendation in Fact-Checking},
  author={Zhang, Wenjia and Gui, Lin and Procter, Rob and He, Yulan},
  journal={arXiv preprint arXiv:2305.04825},
  year={2023}
}",https://arxiv.org/pdf/2305.04825.pdf
34,Zhaoyue Sun,Leveraging ChatGPT in Pharmacovigilance Event Extraction,An empirical study that evaluates ChatGPT's ability on pharmacovigilance event extraction. Several zero-shot and few-shot solutions are proposed and compared with finetuned small models. We also evaluated the effect of introducing ChatGPT generated instances for data augmentation. ,,https://github.com/ZhaoyueSun/phee-with-chatgpt,,,,,"@article{sun2024leveraging,
  title={Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study},
  author={Sun, Zhaoyue and Pergola, Gabriele and Wallace, Byron C and He, Yulan},
  journal={arXiv preprint arXiv:2402.15663},
  year={2024}
}",https://arxiv.org/pdf/2402.15663.pdf
35,Xinyu Wang,Unified Task Embeddings Across Multiple Models,"A framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables comparison and analysis of similarities amongst different models, broadening the scope and utility of existing task embedding methods in multi-model scenarios, while maintaining their performance comparable to architecture-specific methods.",,https://github.com/xnyuwg/fute,,,,,"@inproceedings{wang-etal-2024-towards-unified,
    title = ""Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond"",
    author = ""Wang, Xinyu  and
      Xu, Hainiu  and
      Gui, Lin  and
      He, Yulan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL 2024"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.findings-acl.493/"",
    doi = ""10.18653/v1/2024.findings-acl.493"",
    pages = ""8324--8340"",
}",https://aclanthology.org/2024.findings-acl.493/
36,Linhai Zhang,PROgressive PERsonalization (PROPER),"PROgressive PERsonalization (PROPER), a novel progressive learning framework inspired by meso-level theory in social science. PROPER bridges population-level and user-level models by grouping users based on preferences and adapting LLMs in stages. It combines a Mixture-of-Experts (MoE) structure with Low Ranked Adaptation (LoRA), using a user-aware router to assign users to appropriate groups automatically. Additionally, a LoRA-aware router is proposed to facilitate the integration of individual user LoRAs with group-level LoRAs.",,https://anonymous.4open.science/r/PROPER-63E5/,,,,,"@misc{zhang2025properprogressivelearningframework,
      title={PROPER: A Progressive Learning Framework for Personalized Large Language Models with Group-Level Adaptation}, 
      author={Linhai Zhang and Jialong Wu and Deyu Zhou and Yulan He},
      year={2025},
      eprint={2503.01303},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.01303}, 
}",https://arxiv.org/pdf/2503.01303
37,Yanzheng Xiang,Information-Augmented and ConsistencyEnhanced fine-tuning approach (InfoAC,"We propose a novel Information-Augmented and ConsistencyEnhanced fine-tuning approach to alleviate the sensitivity of CausalLMs to the order of in-context examples.
",,https://github.com/xyzCS/InfoAC,"6 paper citation, 3 stars",,,,"@inproceedings{xiang-etal-2024-addressing,
    title = ""Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models"",
    author = ""Xiang, Yanzheng  and
      Yan, Hanqi  and
      Gui, Lin  and
      He, Yulan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL 2024"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.findings-acl.386/"",
    doi = ""10.18653/v1/2024.findings-acl.386"",
    pages = ""6467--6481""
}",https://arxiv.org/abs/2402.15637
38,Zhaoyue Sun,ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language,ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language. A set of models that generate natural language explanations for DDI predictions.,,https://github.com/ZhaoyueSun/ExDDI,"0 paper citation, 0 stars",,,,"@article{sun2024exddi,
  title={ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language},
  author={Sun, Zhaoyue and Li, Jiazheng and Pergola, Gabriele and He, Yulan},
  journal={arXiv preprint arXiv:2409.05592},
  year={2024}
}",https://arxiv.org/abs/2409.05592
39,Hanqi Yan,"Mirror(Multiple-perspective self-
reflection method for knowledge-rich reasoning)","While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. ",,https://github.com/hanqi-qi/Mirror,"6 paper citation, 12 star",,,,"@inproceedings{yan-etal-2024-mirror,
    title = ""Mirror: Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning"",
    author = ""Yan, Hanqi  and
      Zhu, Qinglin  and
      Wang, Xinyu  and
      Gui, Lin  and
      He, Yulan"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.382/"",
    doi = ""10.18653/v1/2024.acl-long.382"",
    pages = ""7086--7103"",
}",https://aclanthology.org/2024.acl-long.382/
40,Hanqi Yan,Revisit Monosemanticity from a Feature Decorrelation Perspective,"To better interpret the intrinsic mechanism of large language models (LLMs), recent studies focus on monosemanticity on its basic units. A monosemantic neuron is dedicated to a single and specific concept, which forms a one-to-one correlation between neurons and concepts. Despite extensive research in monosemanticity probing, it remains unclear whether monosemanticity is beneficial or harmful to model capacity. To explore this question, we revisit monosemanticity from the feature decorrelation perspective and advocate for its encouragement.",,https://github.com/hanqi-qi/Revisit_monosemanticity,"5 paper citation, 2 star",,,,"@inproceedings{yan-etal-2024-encourage,
    title = ""Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective"",
    author = ""Yan, Hanqi  and
      Xiang, Yanzheng  and
      Chen, Guangyi  and
      Wang, Yifei  and
      Gui, Lin  and
      He, Yulan"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.582/"",
    doi = ""10.18653/v1/2024.emnlp-main.582"",
    pages = ""10423--10435"",
}",https://aclanthology.org/2024.emnlp-main.582/
41,Hanqi Yan,The Multi-faceted Monosemanticity in Multimodal Representations,"We leverage recent advancements in feature monosemanticity to extract interpretable features from deep multi-modal models, offering a data-driven understanding of modality gaps. ",,,0 paper citation,,,,"@inproceedings{
yan2024the,
title={The Multi-faceted Monosemanticity in Multimodal Representations},
author={Hanqi Yan and Yulan He and Yifei Wang},
booktitle={Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models},
year={2024},
url={https://openreview.net/forum?id=9NLRpwfLnT}
}",https://openreview.net/forum?id=9NLRpwfLnT